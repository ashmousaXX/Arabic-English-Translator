{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashmousaXX/Arabic-English-Translator/blob/main/Pattern_Final_Project_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5oiDbZXe2CQ"
      },
      "source": [
        "# **Task 1 (Rawan) : Preprocessing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h0JxT4Xpex7L"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import os\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj1rh0aPeuo9"
      },
      "source": [
        "**Verify dataset exists**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWKNcX-4e8HB",
        "outputId": "035f7a73-2f46-490d-b203-7db5d2f5f8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset from GitHub...\n",
            "✓ Dataset downloaded successfully: content/ara_.txt\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"content/ara_.txt\"\n",
        "\n",
        "# Ensure the directory exists before downloading\n",
        "if not os.path.exists(os.path.dirname(dataset_path)) and os.path.dirname(dataset_path) != '':\n",
        "    os.makedirs(os.path.dirname(dataset_path), exist_ok=True)\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(\"Downloading dataset from GitHub...\")\n",
        "    url = \"https://raw.githubusercontent.com/SamirMoustafa/nmt-with-attention-for-ar-to-en/master/ara_.txt\"\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(f\"✓ Dataset downloaded successfully: {dataset_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hif96gt8e-y0"
      },
      "source": [
        "**Preview first 10 lines**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZNrdSmsfB_K",
        "outputId": "950fad1d-1699-4d7f-e97c-745b707d4487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 lines of the dataset:\n",
            "Line 1: 'Hi.\\tمرحبًا.\\n'\n",
            "Line 2: 'Run!\\tاركض!\\n'\n",
            "Line 3: 'Help!\\tالنجدة!\\n'\n",
            "Line 4: 'Jump!\\tاقفز!\\n'\n",
            "Line 5: 'Stop!\\tقف!\\n'\n",
            "Line 6: 'Go on.\\tداوم.\\n'\n",
            "Line 7: 'Go on.\\tاستمر.\\n'\n",
            "Line 8: 'Hello!\\tمرحباً.\\n'\n",
            "Line 9: 'Hurry!\\tتعجّل!\\n'\n",
            "Line 10: 'Hurry!\\tاستعجل!\\n'\n"
          ]
        }
      ],
      "source": [
        "print(\"First 10 lines of the dataset:\")\n",
        "with open(dataset_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "    lines = f.readlines()\n",
        "    for i, line in enumerate(lines[:10]):\n",
        "        print(f\"Line {i+1}:\", repr(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyyMqwl3fE9M"
      },
      "source": [
        "**Data cleaning functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tzZqQAEGfJMd"
      },
      "outputs": [],
      "source": [
        "def normalize_arabic(text):\n",
        "    text = str(text).strip()\n",
        "\n",
        "    # Remove diacritics only\n",
        "    text = re.sub(r\"[\\u0617-\\u061A\\u064B-\\u0652]\", \"\", text)\n",
        "\n",
        "    # Normalize Alef variants ONLY\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "\n",
        "    # Remove non-Arabic characters\n",
        "    text = re.sub(r\"[^ء-ي0-9\\s]\", \"\", text)\n",
        "\n",
        "    # Normalize spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_english(text):\n",
        "    text = str(text).strip().lower()\n",
        "    text = re.sub(r\"[^a-z0-9.,!?\\'\\\" ]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d-AtJcUfMEL"
      },
      "source": [
        "**Load and clean dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qZnJRl2ffR-v"
      },
      "outputs": [],
      "source": [
        "MAX_AR_LEN = 80\n",
        "MAX_EN_LEN = 80\n",
        "MIN_LEN = 2\n",
        "\n",
        "def load_dataset(path, separator=\"\\t\"):\n",
        "    arabic_sentences = []\n",
        "    english_sentences = []\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(separator)\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "\n",
        "            en, ar = parts\n",
        "            en = clean_english(en)\n",
        "            ar = normalize_arabic(ar)\n",
        "\n",
        "            if not en or not ar:\n",
        "                continue\n",
        "\n",
        "            # Length filtering (word-based)\n",
        "            if not (MIN_LEN <= len(ar.split()) <= MAX_AR_LEN):\n",
        "                continue\n",
        "            if not (MIN_LEN <= len(en.split()) <= MAX_EN_LEN):\n",
        "                continue\n",
        "\n",
        "            arabic_sentences.append(ar)\n",
        "            english_sentences.append(en)\n",
        "\n",
        "    if len(arabic_sentences) == 0:\n",
        "        raise ValueError(\"Dataset is empty or file format is wrong!\")\n",
        "\n",
        "    return arabic_sentences, english_sentences\n",
        "\n",
        "def split_data(ar, en, train_ratio=0.8, val_ratio=0.1, seed=42):\n",
        "    rnd = random.Random(seed)\n",
        "    data = list(zip(ar, en))\n",
        "    rnd.shuffle(data)\n",
        "    ar, en = zip(*data)\n",
        "    n = len(ar)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = int(n * (train_ratio + val_ratio))\n",
        "    return (\n",
        "        ar[:train_end], en[:train_end],\n",
        "        ar[train_end:val_end], en[train_end:val_end],\n",
        "        ar[val_end:], en[val_end:]\n",
        "    )\n",
        "\n",
        "def save_to_file(filename, data):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in data:\n",
        "            f.write(line + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIo-5zz9hW_r",
        "outputId": "8c7af06c-b047-4002-c7ae-1aa19c4b13f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences loaded: 10600\n",
            "Max Arabic length: 36\n",
            "Max English length: 34\n",
            "Avg Arabic length: 4.388301886792453\n",
            "Avg English length: 5.759150943396226\n"
          ]
        }
      ],
      "source": [
        "arabic, english = load_dataset(dataset_path)\n",
        "\n",
        "print(\"Total sentences loaded:\", len(arabic))\n",
        "print(\"Max Arabic length:\", max(len(s.split()) for s in arabic))\n",
        "print(\"Max English length:\", max(len(s.split()) for s in english))\n",
        "print(\"Avg Arabic length:\", sum(len(s.split()) for s in arabic) / len(arabic))\n",
        "print(\"Avg English length:\", sum(len(s.split()) for s in english) / len(english))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DscbT53Ufsmq"
      },
      "source": [
        "**Run preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbLxUFUZfuqJ",
        "outputId": "22475bb1-6912-4207-e9e6-a6170e99767a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 8480\n",
            "Validation: 1060\n",
            "Test: 1060\n",
            "\n",
            "Example:\n",
            "Arabic: هو ترجم الاية الى اللغة الانكليزية\n",
            "English: he translated the verse into english.\n"
          ]
        }
      ],
      "source": [
        "train_ar, train_en, val_ar, val_en, test_ar, test_en = split_data(arabic, english)\n",
        "\n",
        "print(\"Train:\", len(train_ar))\n",
        "print(\"Validation:\", len(val_ar))\n",
        "print(\"Test:\", len(test_ar))\n",
        "print(\"\\nExample:\")\n",
        "print(\"Arabic:\", train_ar[0])\n",
        "print(\"English:\", train_en[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm5a1NDTf9wN"
      },
      "source": [
        "**Save processed files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QNqh89sef_0r"
      },
      "outputs": [],
      "source": [
        "# Save processed files\n",
        "save_to_file(\"train.ar\", train_ar)\n",
        "save_to_file(\"train.en\", train_en)\n",
        "save_to_file(\"val.ar\", val_ar)\n",
        "save_to_file(\"val.en\", val_en)\n",
        "save_to_file(\"test.ar\", test_ar)\n",
        "save_to_file(\"test.en\", test_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xIKD1SDhblo"
      },
      "source": [
        "# **Task 2 (Amal) : TOKENIZING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNsPKm2DhvEV",
        "outputId": "7d38c066-6925-430d-d581-2d073bcdae0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Tokenization...\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "print(\"Starting Tokenization...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvT2psIghzLi"
      },
      "source": [
        "**Train SentencePiece Tokenizers (Arabic + English)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTdTxQLlh3ry",
        "outputId": "2f53522e-e4c6-4a3f-d2b1-e473734f32d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tokenizer trained: arabic_tokenizer.model\n",
            " Tokenizer trained: english_tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "def train_spm(input_file, model_prefix, vocab_size=6000):\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        f\"--input={input_file} \"\n",
        "        f\"--model_prefix={model_prefix} \"\n",
        "        f\"--vocab_size={vocab_size} \"\n",
        "        f\"--model_type=bpe \"\n",
        "        f\"--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\"\n",
        "    )\n",
        "    print(f\" Tokenizer trained: {model_prefix}.model\")\n",
        "\n",
        "train_spm(\"train.ar\", \"arabic_tokenizer\", vocab_size=6000)\n",
        "train_spm(\"train.en\", \"english_tokenizer\", vocab_size=6000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B3ORFQ2h7OJ",
        "outputId": "53ff81aa-1268-41d2-857e-1fb2a00e9342"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Load tokenizers\n",
        "sp_ar = spm.SentencePieceProcessor()\n",
        "sp_en = spm.SentencePieceProcessor()\n",
        "sp_ar.load(\"arabic_tokenizer.model\")\n",
        "sp_en.load(\"english_tokenizer.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfAurx6xiCV5"
      },
      "source": [
        "**Encode datasets to ID sequences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ad0ZtqQ5iFtQ"
      },
      "outputs": [],
      "source": [
        "def encode_file(input_path, output_path, sp):\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f_in, \\\n",
        "         open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "        for line in f_in:\n",
        "            ids = sp.encode(\n",
        "                            line.strip(),\n",
        "                            out_type=int,\n",
        "                            add_bos=True,\n",
        "                            add_eos=True\n",
        "                           )\n",
        "\n",
        "            f_out.write(\" \".join(map(str, ids)) + \"\\n\")\n",
        "\n",
        "    print(f\"Encoded: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBJMMJnCiHEU",
        "outputId": "54174bd9-12aa-4a99-e586-9d19899f4b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: train.ar.ids\n",
            "Encoded: val.ar.ids\n",
            "Encoded: test.ar.ids\n",
            "Encoded: train.en.ids\n",
            "Encoded: val.en.ids\n",
            "Encoded: test.en.ids\n"
          ]
        }
      ],
      "source": [
        "# Encode Arabic\n",
        "encode_file(\"train.ar\", \"train.ar.ids\", sp_ar)\n",
        "encode_file(\"val.ar\", \"val.ar.ids\", sp_ar)\n",
        "encode_file(\"test.ar\", \"test.ar.ids\", sp_ar)\n",
        "\n",
        "# Encode English\n",
        "encode_file(\"train.en\", \"train.en.ids\", sp_en)\n",
        "encode_file(\"val.en\", \"val.en.ids\", sp_en)\n",
        "encode_file(\"test.en\", \"test.en.ids\", sp_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh5ZR6UQdxLN"
      },
      "source": [
        "# **Task 3 Lamis : Transformer Model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gjTtk3MzBBfC"
      },
      "outputs": [],
      "source": [
        "# ==================== COMPLETE TRANSFORMER MODEL ====================\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ========== SPECIAL TOKENS ==========\n",
        "PAD_ID = 0\n",
        "UNK_ID = 1\n",
        "BOS_ID = 2\n",
        "EOS_ID = 3\n",
        "\n",
        "# ========== POSITIONAL ENCODING ==========\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
        "        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model)\n",
        "    )\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "# ========== MASKING FUNCTIONS (Keras MultiHeadAttention-compatible) ==========\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    \"\"\"\n",
        "    Returns mask for keys: 1 for real tokens, 0 for PAD.\n",
        "    Shape: (batch, 1, seq_len) -> will broadcast to (batch, query_len, seq_len)\n",
        "    \"\"\"\n",
        "    mask = tf.cast(tf.not_equal(seq, PAD_ID), tf.float32)  # (B, S)\n",
        "    return mask[:, tf.newaxis, :]                          # (B, 1, S)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"\n",
        "    Causal mask: 1 where allowed, 0 where future is blocked.\n",
        "    Shape: (size, size)\n",
        "    \"\"\"\n",
        "    return tf.linalg.band_part(tf.ones((size, size), dtype=tf.float32), -1, 0)\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      enc_self_mask: (B, 1, S_src)   broadcasted for encoder self-attn\n",
        "      dec_self_mask: (B, T_tgt, T_tgt) for decoder self-attn (causal + padding)\n",
        "      dec_cross_mask:(B, 1, S_src)   broadcasted for cross-attn keys (encoder output)\n",
        "    \"\"\"\n",
        "    # Encoder self-attention mask (key padding only)\n",
        "    enc_self_mask = create_padding_mask(inp)   # (B, 1, S_src)\n",
        "\n",
        "    # Decoder cross-attention mask (mask encoder keys)\n",
        "    dec_cross_mask = create_padding_mask(inp)  # (B, 1, S_src)\n",
        "\n",
        "    # Decoder self-attention mask = causal * target_key_padding\n",
        "    tgt_key_padding = create_padding_mask(tar) # (B, 1, T_tgt)\n",
        "    T = tf.shape(tar)[1]\n",
        "    causal = create_look_ahead_mask(T)         # (T_tgt, T_tgt)\n",
        "\n",
        "    # Broadcast tgt padding to (B, T_tgt, T_tgt) and apply causal\n",
        "    dec_self_mask = tgt_key_padding * causal[tf.newaxis, :, :]  # (B, T_tgt, T_tgt)\n",
        "\n",
        "    return enc_self_mask, dec_self_mask, dec_cross_mask\n",
        "\n",
        "# ========== FEED FORWARD NETWORK ==========\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        layers.Dense(dff, activation='relu'),\n",
        "        layers.Dense(d_model)\n",
        "    ])\n",
        "\n",
        "# ========== ENCODER LAYER ==========\n",
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "    num_heads=num_heads,\n",
        "    key_dim=d_model // num_heads\n",
        ")\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "      attn_output = self.mha(\n",
        "          query=x, value=x, key=x,\n",
        "          attention_mask=mask\n",
        "      )\n",
        "      attn_output = self.dropout1(attn_output, training=training)\n",
        "      out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "      ffn_output = self.ffn(out1)\n",
        "      ffn_output = self.dropout2(ffn_output, training=training)\n",
        "      out2 = self.layernorm2(out1 + ffn_output)\n",
        "      return out2\n",
        "\n",
        "\n",
        "# ========== ENCODER ==========\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x=x, training=training, mask=mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ========== DECODER LAYER ==========\n",
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha1 = layers.MultiHeadAttention(\n",
        "    num_heads=num_heads,\n",
        "    key_dim=d_model // num_heads\n",
        ")\n",
        "        self.mha2 = layers.MultiHeadAttention(\n",
        "    num_heads=num_heads,\n",
        "    key_dim=d_model // num_heads\n",
        ")\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.dropout3 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "      # Self-attention (causal + target padding)\n",
        "      attn1 = self.mha1(\n",
        "          query=x, value=x, key=x,\n",
        "          attention_mask=look_ahead_mask\n",
        "      )\n",
        "      attn1 = self.dropout1(attn1, training=training)\n",
        "      out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "      # Cross-attention (mask encoder padding on keys)\n",
        "      attn2 = self.mha2(\n",
        "          query=out1, value=enc_output, key=enc_output,\n",
        "          attention_mask=padding_mask\n",
        "      )\n",
        "      attn2 = self.dropout2(attn2, training=training)\n",
        "      out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "      ffn_output = self.ffn(out2)\n",
        "      ffn_output = self.dropout3(ffn_output, training=training)\n",
        "      out3 = self.layernorm3(ffn_output + out2)\n",
        "      return out3\n",
        "\n",
        "# ========== DECODER ==========\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.dec_layers[i](\n",
        "                x=x,\n",
        "                enc_output=enc_output,\n",
        "                training=training,\n",
        "                look_ahead_mask=look_ahead_mask,\n",
        "                padding_mask=padding_mask\n",
        "            )\n",
        "\n",
        "        return x\n",
        "\n",
        "# ========== COMPLETE TRANSFORMER ==========\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, target_vocab_size,\n",
        "                 pe_input, pe_target, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training=False, enc_padding_mask=None,\n",
        "             look_ahead_mask=None, dec_padding_mask=None):\n",
        "\n",
        "        enc_output = self.encoder(\n",
        "            x=inp,\n",
        "            training=training,\n",
        "            mask=enc_padding_mask\n",
        "        )\n",
        "\n",
        "        dec_output = self.decoder(\n",
        "            x=tar,\n",
        "            enc_output=enc_output,\n",
        "            training=training,\n",
        "            look_ahead_mask=look_ahead_mask,\n",
        "            padding_mask=dec_padding_mask\n",
        "        )\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "# ========== MODEL HYPERPARAMETERS ==========\n",
        "NUM_LAYERS = 4\n",
        "D_MODEL = 256\n",
        "DFF = 1024\n",
        "NUM_HEADS = 8\n",
        "DROPOUT_RATE = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umoZlQl9-rfg"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qYqTUJ7B3kI",
        "outputId": "ea74608e-85ef-4c01-a4b6-b425d853b77e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 8480\n",
            "Validation samples: 1060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-1402922802.py:44: bucket_by_sequence_length (from tensorflow.python.data.experimental.ops.grouping) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.bucket_by_sequence_length(...)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built: 11,986,800 parameters\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 8.6475\n",
            "  Batch 20: loss = 8.5674\n",
            "  Batch 40: loss = 8.3535\n",
            "  Batch 60: loss = 8.0531\n",
            "  Batch 80: loss = 7.7623\n",
            "  Batch 100: loss = 7.5341\n",
            "  Batch 120: loss = 7.2707\n",
            "  Train Loss: 8.0028\n",
            "  Val Loss: 7.2661\n",
            "\n",
            "Epoch 2/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 7.0685\n",
            "  Batch 20: loss = 6.8610\n",
            "  Batch 40: loss = 6.5680\n",
            "  Batch 60: loss = 6.3239\n",
            "  Batch 80: loss = 6.1022\n",
            "  Batch 100: loss = 5.8753\n",
            "  Batch 120: loss = 5.5792\n",
            "  Train Loss: 6.3595\n",
            "  Val Loss: 5.7608\n",
            "  Model saved\n",
            "\n",
            "Epoch 3/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 5.3894\n",
            "  Batch 20: loss = 5.7991\n",
            "  Batch 40: loss = 5.0638\n",
            "  Batch 60: loss = 4.6710\n",
            "  Batch 80: loss = 4.5743\n",
            "  Batch 100: loss = 4.4003\n",
            "  Batch 120: loss = 4.5547\n",
            "  Train Loss: 4.9258\n",
            "  Val Loss: 4.9193\n",
            "\n",
            "Epoch 4/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 4.3720\n",
            "  Batch 20: loss = 4.9142\n",
            "  Batch 40: loss = 4.3088\n",
            "  Batch 60: loss = 4.1338\n",
            "  Batch 80: loss = 4.1440\n",
            "  Batch 100: loss = 3.7898\n",
            "  Batch 120: loss = 3.8992\n",
            "  Train Loss: 4.2062\n",
            "  Val Loss: 4.5506\n",
            "  Model saved\n",
            "\n",
            "Epoch 5/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 3.9397\n",
            "  Batch 20: loss = 3.8090\n",
            "  Batch 40: loss = 3.5682\n",
            "  Batch 60: loss = 3.8224\n",
            "  Batch 80: loss = 3.6473\n",
            "  Batch 100: loss = 3.3562\n",
            "  Batch 120: loss = 3.4511\n",
            "  Train Loss: 3.7365\n",
            "  Val Loss: 4.3733\n",
            "\n",
            "Epoch 6/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 3.4236\n",
            "  Batch 20: loss = 4.2147\n",
            "  Batch 40: loss = 3.4052\n",
            "  Batch 60: loss = 3.2827\n",
            "  Batch 80: loss = 3.1757\n",
            "  Batch 100: loss = 3.2377\n",
            "  Batch 120: loss = 3.1465\n",
            "  Train Loss: 3.3766\n",
            "  Val Loss: 4.1050\n",
            "  Model saved\n",
            "\n",
            "Epoch 7/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 2.9875\n",
            "  Batch 20: loss = 3.0777\n",
            "  Batch 40: loss = 2.8256\n",
            "  Batch 60: loss = 3.0267\n",
            "  Batch 80: loss = 2.7508\n",
            "  Batch 100: loss = 2.8255\n",
            "  Batch 120: loss = 2.9046\n",
            "  Train Loss: 3.0585\n",
            "  Val Loss: 4.0514\n",
            "\n",
            "Epoch 8/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 2.8560\n",
            "  Batch 20: loss = 3.6751\n",
            "  Batch 40: loss = 2.5803\n",
            "  Batch 60: loss = 3.4899\n",
            "  Batch 80: loss = 2.5974\n",
            "  Batch 100: loss = 2.5730\n",
            "  Batch 120: loss = 2.3666\n",
            "  Train Loss: 2.7733\n",
            "  Val Loss: 3.8475\n",
            "  Model saved\n",
            "\n",
            "Epoch 9/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 2.5781\n",
            "  Batch 20: loss = 2.3315\n",
            "  Batch 40: loss = 2.2245\n",
            "  Batch 60: loss = 2.3596\n",
            "  Batch 80: loss = 2.3796\n",
            "  Batch 100: loss = 2.3448\n",
            "  Batch 120: loss = 2.1780\n",
            "  Train Loss: 2.5108\n",
            "  Val Loss: 3.8150\n",
            "\n",
            "Epoch 10/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 2.1544\n",
            "  Batch 20: loss = 2.1025\n",
            "  Batch 40: loss = 2.1182\n",
            "  Batch 60: loss = 2.0440\n",
            "  Batch 80: loss = 2.0852\n",
            "  Batch 100: loss = 2.2425\n",
            "  Batch 120: loss = 2.1287\n",
            "  Train Loss: 2.2680\n",
            "  Val Loss: 3.7646\n",
            "  Model saved\n",
            "\n",
            "Epoch 11/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 1.7996\n",
            "  Batch 20: loss = 1.9580\n",
            "  Batch 40: loss = 1.8944\n",
            "  Batch 60: loss = 1.6836\n",
            "  Batch 80: loss = 1.7206\n",
            "  Batch 100: loss = 1.7810\n",
            "  Batch 120: loss = 1.6540\n",
            "  Train Loss: 2.0198\n",
            "  Val Loss: 3.8177\n",
            "\n",
            "Epoch 12/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 1.8706\n",
            "  Batch 20: loss = 1.6463\n",
            "  Batch 40: loss = 1.6770\n",
            "  Batch 60: loss = 1.6602\n",
            "  Batch 80: loss = 1.6371\n",
            "  Batch 100: loss = 1.7059\n",
            "  Batch 120: loss = 1.6628\n",
            "  Train Loss: 1.7985\n",
            "  Val Loss: 3.7105\n",
            "  Model saved\n",
            "\n",
            "Epoch 13/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 1.3990\n",
            "  Batch 20: loss = 1.4071\n",
            "  Batch 40: loss = 1.4878\n",
            "  Batch 60: loss = 1.3515\n",
            "  Batch 80: loss = 2.2023\n",
            "  Batch 100: loss = 1.4709\n",
            "  Batch 120: loss = 1.4393\n",
            "  Train Loss: 1.5647\n",
            "  Val Loss: 3.8171\n",
            "\n",
            "Epoch 14/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 1.4638\n",
            "  Batch 20: loss = 1.2067\n",
            "  Batch 40: loss = 1.0596\n",
            "  Batch 60: loss = 1.1313\n",
            "  Batch 80: loss = 1.2846\n",
            "  Batch 100: loss = 1.3305\n",
            "  Batch 120: loss = 1.0840\n",
            "  Train Loss: 1.3407\n",
            "  Val Loss: 3.5090\n",
            "  Model saved\n",
            "\n",
            "Epoch 15/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 1.2303\n",
            "  Batch 20: loss = 1.0334\n",
            "  Batch 40: loss = 0.9734\n",
            "  Batch 60: loss = 0.9946\n",
            "  Batch 80: loss = 0.9283\n",
            "  Batch 100: loss = 0.9417\n",
            "  Batch 120: loss = 1.0057\n",
            "  Train Loss: 1.1445\n",
            "  Val Loss: 3.6961\n",
            "\n",
            "Epoch 16/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.9319\n",
            "  Batch 20: loss = 0.8310\n",
            "  Batch 40: loss = 0.8355\n",
            "  Batch 60: loss = 0.8799\n",
            "  Batch 80: loss = 0.7934\n",
            "  Batch 100: loss = 0.7685\n",
            "  Batch 120: loss = 0.9052\n",
            "  Train Loss: 0.9667\n",
            "  Val Loss: 3.7176\n",
            "  Model saved\n",
            "\n",
            "Epoch 17/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.8013\n",
            "  Batch 20: loss = 0.6889\n",
            "  Batch 40: loss = 1.3067\n",
            "  Batch 60: loss = 1.2327\n",
            "  Batch 80: loss = 0.6091\n",
            "  Batch 100: loss = 0.6762\n",
            "  Batch 120: loss = 0.7205\n",
            "  Train Loss: 0.8043\n",
            "  Val Loss: 3.6932\n",
            "\n",
            "Epoch 18/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.6732\n",
            "  Batch 20: loss = 0.5439\n",
            "  Batch 40: loss = 0.5284\n",
            "  Batch 60: loss = 0.5595\n",
            "  Batch 80: loss = 0.5635\n",
            "  Batch 100: loss = 0.6248\n",
            "  Batch 120: loss = 0.5968\n",
            "  Train Loss: 0.6753\n",
            "  Val Loss: 3.7172\n",
            "  Model saved\n",
            "\n",
            "Epoch 19/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.5316\n",
            "  Batch 20: loss = 0.4830\n",
            "  Batch 40: loss = 0.4884\n",
            "  Batch 60: loss = 0.4968\n",
            "  Batch 80: loss = 0.5100\n",
            "  Batch 100: loss = 0.4851\n",
            "  Batch 120: loss = 0.4817\n",
            "  Train Loss: 0.5682\n",
            "  Val Loss: 3.5966\n",
            "\n",
            "Epoch 20/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.4595\n",
            "  Batch 20: loss = 0.6867\n",
            "  Batch 40: loss = 0.3942\n",
            "  Batch 60: loss = 0.4540\n",
            "  Batch 80: loss = 0.4554\n",
            "  Batch 100: loss = 0.3983\n",
            "  Batch 120: loss = 0.4021\n",
            "  Train Loss: 0.4805\n",
            "  Val Loss: 3.8427\n",
            "  Model saved\n",
            "\n",
            "Epoch 21/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.3585\n",
            "  Batch 20: loss = 0.3521\n",
            "  Batch 40: loss = 0.3168\n",
            "  Batch 60: loss = 0.5726\n",
            "  Batch 80: loss = 0.6476\n",
            "  Batch 100: loss = 0.4224\n",
            "  Batch 120: loss = 0.4511\n",
            "  Train Loss: 0.3980\n",
            "  Val Loss: 3.8156\n",
            "\n",
            "Epoch 22/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.4469\n",
            "  Batch 20: loss = 0.2952\n",
            "  Batch 40: loss = 0.5496\n",
            "  Batch 60: loss = 0.3168\n",
            "  Batch 80: loss = 0.3449\n",
            "  Batch 100: loss = 0.3165\n",
            "  Batch 120: loss = 0.3610\n",
            "  Train Loss: 0.3667\n",
            "  Val Loss: 3.9239\n",
            "  Model saved\n",
            "\n",
            "Epoch 23/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.2348\n",
            "  Batch 20: loss = 0.2491\n",
            "  Batch 40: loss = 0.2519\n",
            "  Batch 60: loss = 0.2370\n",
            "  Batch 80: loss = 0.3742\n",
            "  Batch 100: loss = 0.2230\n",
            "  Batch 120: loss = 0.2321\n",
            "  Train Loss: 0.3120\n",
            "  Val Loss: 4.0036\n",
            "\n",
            "Epoch 24/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.2934\n",
            "  Batch 20: loss = 0.2267\n",
            "  Batch 40: loss = 0.1994\n",
            "  Batch 60: loss = 0.4923\n",
            "  Batch 80: loss = 0.2085\n",
            "  Batch 100: loss = 0.4467\n",
            "  Batch 120: loss = 0.2124\n",
            "  Train Loss: 0.2990\n",
            "  Val Loss: 4.1180\n",
            "  Model saved\n",
            "\n",
            "Epoch 25/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.2458\n",
            "  Batch 20: loss = 0.2735\n",
            "  Batch 40: loss = 0.2684\n",
            "  Batch 60: loss = 0.2140\n",
            "  Batch 80: loss = 0.2320\n",
            "  Batch 100: loss = 0.2514\n",
            "  Batch 120: loss = 0.2405\n",
            "  Train Loss: 0.2715\n",
            "  Val Loss: 4.0580\n",
            "\n",
            "Epoch 26/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.4049\n",
            "  Batch 20: loss = 0.2357\n",
            "  Batch 40: loss = 0.1675\n",
            "  Batch 60: loss = 0.2660\n",
            "  Batch 80: loss = 0.2873\n",
            "  Batch 100: loss = 0.2580\n",
            "  Batch 120: loss = 0.2318\n",
            "  Train Loss: 0.2608\n",
            "  Val Loss: 4.1881\n",
            "  Model saved\n",
            "\n",
            "Epoch 27/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.3024\n",
            "  Batch 20: loss = 0.1844\n",
            "  Batch 40: loss = 0.2151\n",
            "  Batch 60: loss = 0.2158\n",
            "  Batch 80: loss = 0.2232\n",
            "  Batch 100: loss = 0.1789\n",
            "  Batch 120: loss = 0.2496\n",
            "  Train Loss: 0.2492\n",
            "  Val Loss: 4.2163\n",
            "\n",
            "Epoch 28/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.3126\n",
            "  Batch 20: loss = 0.2539\n",
            "  Batch 40: loss = 0.2008\n",
            "  Batch 60: loss = 0.1691\n",
            "  Batch 80: loss = 0.2323\n",
            "  Batch 100: loss = 0.2437\n",
            "  Batch 120: loss = 0.2631\n",
            "  Train Loss: 0.2425\n",
            "  Val Loss: 4.2776\n",
            "  Model saved\n",
            "\n",
            "Epoch 29/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.2840\n",
            "  Batch 20: loss = 0.1778\n",
            "  Batch 40: loss = 0.2375\n",
            "  Batch 60: loss = 0.1276\n",
            "  Batch 80: loss = 0.2134\n",
            "  Batch 100: loss = 0.2766\n",
            "  Batch 120: loss = 0.1950\n",
            "  Train Loss: 0.2309\n",
            "  Val Loss: 4.3155\n",
            "\n",
            "Epoch 30/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.1982\n",
            "  Batch 20: loss = 0.3111\n",
            "  Batch 40: loss = 0.1965\n",
            "  Batch 60: loss = 0.1833\n",
            "  Batch 80: loss = 0.2333\n",
            "  Batch 100: loss = 0.2069\n",
            "  Batch 120: loss = 0.2528\n",
            "  Train Loss: 0.2265\n",
            "  Val Loss: 4.2240\n",
            "  Model saved\n",
            "\n",
            "Epoch 31/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.1764\n",
            "  Batch 20: loss = 0.1307\n",
            "  Batch 40: loss = 0.2209\n",
            "  Batch 60: loss = 0.1562\n",
            "  Batch 80: loss = 0.1692\n",
            "  Batch 100: loss = 0.1649\n",
            "  Batch 120: loss = 0.2038\n",
            "  Train Loss: 0.2072\n",
            "  Val Loss: 4.6163\n",
            "\n",
            "Epoch 32/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.2915\n",
            "  Batch 20: loss = 0.2101\n",
            "  Batch 40: loss = 0.1442\n",
            "  Batch 60: loss = 0.1223\n",
            "  Batch 80: loss = 0.1216\n",
            "  Batch 100: loss = 0.1728\n",
            "  Batch 120: loss = 0.2254\n",
            "  Train Loss: 0.1957\n",
            "  Val Loss: 4.2927\n",
            "  Model saved\n",
            "\n",
            "Epoch 33/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.1381\n",
            "  Batch 20: loss = 0.1787\n",
            "  Batch 40: loss = 0.2040\n",
            "  Batch 60: loss = 0.2294\n",
            "  Batch 80: loss = 0.1502\n",
            "  Batch 100: loss = 0.1776\n",
            "  Batch 120: loss = 0.1631\n",
            "  Train Loss: 0.1737\n",
            "  Val Loss: 4.4859\n",
            "\n",
            "Epoch 34/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.1663\n",
            "  Batch 20: loss = 0.2372\n",
            "  Batch 40: loss = 0.2336\n",
            "  Batch 60: loss = 0.1658\n",
            "  Batch 80: loss = 0.1808\n",
            "  Batch 100: loss = 0.2067\n",
            "  Batch 120: loss = 0.1794\n",
            "  Train Loss: 0.1675\n",
            "  Val Loss: 4.3470\n",
            "  Model saved\n",
            "\n",
            "Epoch 35/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.0897\n",
            "  Batch 20: loss = 0.1649\n",
            "  Batch 40: loss = 0.1742\n",
            "  Batch 60: loss = 0.1724\n",
            "  Batch 80: loss = 0.1418\n",
            "  Batch 100: loss = 0.1376\n",
            "  Batch 120: loss = 0.1356\n",
            "  Train Loss: 0.1556\n",
            "  Val Loss: 4.5586\n",
            "\n",
            "Epoch 36/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.1530\n",
            "  Batch 20: loss = 0.1138\n",
            "  Batch 40: loss = 0.1534\n",
            "  Batch 60: loss = 0.0969\n",
            "  Batch 80: loss = 0.1162\n",
            "  Batch 100: loss = 0.1287\n",
            "  Batch 120: loss = 0.1309\n",
            "  Train Loss: 0.1454\n",
            "  Val Loss: 4.6332\n",
            "  Model saved\n",
            "\n",
            "Epoch 37/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.1274\n",
            "  Batch 20: loss = 0.0999\n",
            "  Batch 40: loss = 0.1143\n",
            "  Batch 60: loss = 0.1347\n",
            "  Batch 80: loss = 0.1109\n",
            "  Batch 100: loss = 0.0730\n",
            "  Batch 120: loss = 0.1085\n",
            "  Train Loss: 0.1315\n",
            "  Val Loss: 4.6084\n",
            "\n",
            "Epoch 38/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.1497\n",
            "  Batch 20: loss = 0.0842\n",
            "  Batch 40: loss = 0.1522\n",
            "  Batch 60: loss = 0.0887\n",
            "  Batch 80: loss = 0.1070\n",
            "  Batch 100: loss = 0.1123\n",
            "  Batch 120: loss = 0.1265\n",
            "  Train Loss: 0.1197\n",
            "  Val Loss: 4.5685\n",
            "  Model saved\n",
            "\n",
            "Epoch 39/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.1333\n",
            "  Batch 20: loss = 0.1076\n",
            "  Batch 40: loss = 0.0670\n",
            "  Batch 60: loss = 0.0798\n",
            "  Batch 80: loss = 0.0561\n",
            "  Batch 100: loss = 0.0555\n",
            "  Batch 120: loss = 0.1342\n",
            "  Train Loss: 0.1100\n",
            "  Val Loss: 4.4917\n",
            "\n",
            "Epoch 40/40\n",
            "----------------------------------------\n",
            "  Batch 0: loss = 0.0828\n",
            "  Batch 20: loss = 0.1117\n",
            "  Batch 40: loss = 0.1179\n",
            "  Batch 60: loss = 0.0891\n",
            "  Batch 80: loss = 0.1158\n",
            "  Batch 100: loss = 0.1207\n",
            "  Batch 120: loss = 0.1149\n",
            "  Train Loss: 0.1089\n",
            "  Val Loss: 4.6839\n",
            "  Model saved\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# ==================== COMPLETE TRAINING PIPELINE - FIXED ====================\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# ========== 1. LOAD TOKENIZED DATA ==========\n",
        "def load_ids_file(filename):\n",
        "    data = []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tokens = [int(x) for x in line.strip().split()]\n",
        "            if tokens:\n",
        "                data.append(tokens)\n",
        "    return data\n",
        "\n",
        "train_src = load_ids_file(\"train.ar.ids\")\n",
        "train_tgt = load_ids_file(\"train.en.ids\")\n",
        "val_src = load_ids_file(\"val.ar.ids\")\n",
        "val_tgt = load_ids_file(\"val.en.ids\")\n",
        "\n",
        "print(f\"Training samples: {len(train_src)}\")\n",
        "print(f\"Validation samples: {len(val_src)}\")\n",
        "\n",
        "# ========== 2. CREATE DATASETS ==========\n",
        "def create_tf_dataset(src_data, tgt_data, shuffle=True):\n",
        "    def generator():\n",
        "        for src, tgt in zip(src_data, tgt_data):\n",
        "            yield (np.array(src, dtype=np.int32),\n",
        "                   np.array(tgt, dtype=np.int32))\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(10000)\n",
        "\n",
        "    # 🔥 BUCKET BY SEQUENCE LENGTH (SOURCE LENGTH)\n",
        "    dataset = dataset.apply(\n",
        "        tf.data.experimental.bucket_by_sequence_length(\n",
        "            element_length_func=lambda src, tgt: tf.shape(src)[0],\n",
        "            bucket_boundaries=[10, 20, 30, 40, 50],\n",
        "            bucket_batch_sizes=[64, 64, 32, 32, 16, 8],\n",
        "            padded_shapes=([None], [None]),\n",
        "            padding_values=(PAD_ID, PAD_ID),\n",
        "            drop_remainder=False\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "train_dataset = create_tf_dataset(train_src, train_tgt, shuffle=True)\n",
        "val_dataset = create_tf_dataset(val_src, val_tgt, shuffle=False)\n",
        "\n",
        "# ========== 3. INITIALIZE MODEL ==========\n",
        "input_vocab_size = sp_ar.get_piece_size()\n",
        "target_vocab_size = sp_en.get_piece_size()\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=NUM_LAYERS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dff=DFF,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=DROPOUT_RATE\n",
        ")\n",
        "\n",
        "# ========== 4. BUILD MODEL ==========\n",
        "# Test with one batch to build model\n",
        "for inp, tar in train_dataset.take(1):\n",
        "    tar_input = tar[:, :-1]\n",
        "    enc_mask, look_ahead_mask, dec_mask = create_masks(inp, tar_input)\n",
        "\n",
        "    # FIX: Use keyword arguments here too!\n",
        "    _ = transformer(\n",
        "        inp=inp,\n",
        "        tar=tar_input,\n",
        "        training=False,\n",
        "        enc_padding_mask=enc_mask,\n",
        "        look_ahead_mask=look_ahead_mask,\n",
        "        dec_padding_mask=dec_mask\n",
        "    )\n",
        "    break\n",
        "\n",
        "print(f\"Model built: {transformer.count_params():,} parameters\")\n",
        "\n",
        "# ========== 5. OPTIMIZER & LOSS ==========\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super().__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(tf.maximum(step, 1.0))\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9\n",
        ")\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none'\n",
        ")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.not_equal(real, PAD_ID)\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.maximum(tf.reduce_sum(mask), 1.0)\n",
        "\n",
        "# ========== 6. TRAINING FUNCTIONS (FIXED) ==========\n",
        "@tf.function(reduce_retracing=True)\n",
        "def train_step(inp, tar):\n",
        "    tar_input = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_input)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = transformer(\n",
        "            inp=inp,\n",
        "            tar=tar_input,\n",
        "            training=True,\n",
        "            enc_padding_mask=enc_padding_mask,\n",
        "            look_ahead_mask=look_ahead_mask,\n",
        "            dec_padding_mask=dec_padding_mask\n",
        "        )\n",
        "\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "@tf.function(reduce_retracing=True)\n",
        "def val_step(inp, tar):\n",
        "    tar_input = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_input)\n",
        "\n",
        "    predictions = transformer(\n",
        "        inp=inp,\n",
        "        tar=tar_input,\n",
        "        training=False,\n",
        "        enc_padding_mask=enc_padding_mask,\n",
        "        look_ahead_mask=look_ahead_mask,\n",
        "        dec_padding_mask=dec_padding_mask\n",
        "    )\n",
        "\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# ========== 7. SIMPLE TRAINING LOOP ==========\n",
        "def simple_train(epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Training\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        for batch_idx, (inp_batch, tar_batch) in enumerate(train_dataset):\n",
        "            batch_loss = train_step(inp_batch, tar_batch)\n",
        "            train_loss += batch_loss\n",
        "            train_batches += 1\n",
        "\n",
        "            if batch_idx % 20 == 0:\n",
        "                print(f\"  Batch {batch_idx}: loss = {batch_loss.numpy():.4f}\")\n",
        "\n",
        "        avg_train = train_loss / train_batches\n",
        "        print(f\"  Train Loss: {avg_train.numpy():.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "\n",
        "        for batch_idx, (inp_batch, tar_batch) in enumerate(val_dataset):\n",
        "            batch_loss = val_step(inp_batch, tar_batch)\n",
        "            val_loss += batch_loss\n",
        "            val_batches += 1\n",
        "\n",
        "        avg_val = val_loss / val_batches\n",
        "        print(f\"  Val Loss: {avg_val.numpy():.4f}\")\n",
        "\n",
        "        # Save\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            transformer.save_weights(f\"transformer_epoch_{epoch+1}.weights.h5\")\n",
        "            print(f\"  Model saved\")\n",
        "\n",
        "    transformer.save_weights(\"transformer_final.weights.h5\")\n",
        "    print(\"\\nTraining complete!\")\n",
        "\n",
        "# ========== 8. START TRAINING ==========\n",
        "print(\"\\nStarting training...\")\n",
        "simple_train(epochs=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H7D0K-qDHre"
      },
      "source": [
        "**Checking Point**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Success indicators:\n",
        "\n",
        "        Loss decreases gradually with each epoch\n",
        "   \n",
        "        Not all gradients are zero\n",
        "\n",
        "        Predictions do not contain NaN or Infinity\n",
        "\n",
        "        The model saves weights (.h5 files are available)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJDld-d3NAYJ"
      },
      "source": [
        "\n",
        "Check Loss function pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y-YTgp9NMtp"
      },
      "source": [
        "TEST TRANSLATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnxpwZ4nNLB6",
        "outputId": "ecf166d3-18ab-42fd-c9e7-c130fbc20fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "QUICK TRANSLATION TEST\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "RANDOM TEST SET TRANSLATION (GREEDY)\n",
            "============================================================\n",
            "\n",
            "AR:   توم ينظف اسنانه بالفرشاة ثلاث مرات على الاقل يوميا\n",
            "PRED: tom spends a dentist appointment on his times.\n",
            "REF:  tom brushes his teeth at least three times a day.\n",
            "\n",
            "AR:   نصحته الا يضع الكثير من السكر\n",
            "PRED: she advised him not to drive too much sugar.\n",
            "REF:  she advised him not to use too much sugar.\n",
            "\n",
            "AR:   ايمكنني الذهاب الى النهر\n",
            "PRED: can i go to the river a river?\n",
            "REF:  may i go to the river?\n",
            "\n",
            "AR:   ما يحتاجه حقا هو وظيفة جيدة\n",
            "PRED: what really needed is a good job.\n",
            "REF:  what he needs most is a good job.\n",
            "\n",
            "AR:   من الممكن انها لن تاتي غدا\n",
            "PRED: she may never come here tomorrow.\n",
            "REF:  she may not come here tomorrow.\n",
            "\n",
            "AR:   اتى الولد راكضا\n",
            "PRED: the boy began to cry.\n",
            "REF:  the boy came running.\n",
            "\n",
            "AR:   لعل ذلك صحيح\n",
            "PRED: i guess is true.\n",
            "REF:  perhaps that's true.\n",
            "\n",
            "AR:   اعدك بذلك\n",
            "PRED: i promise this.\n",
            "REF:  i give you my word.\n",
            "\n",
            "AR:   هل جننت\n",
            "PRED: are you studying?\n",
            "REF:  have you gone nuts?\n",
            "\n",
            "AR:   لا يرغب توم في بيع مزرعته\n",
            "PRED: tom doesn't want to agree to do the farm.\n",
            "REF:  tom doesn't want to sell his farm.\n"
          ]
        }
      ],
      "source": [
        "# ========== TEST TRANSLATION IMMEDIATELY ==========\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"QUICK TRANSLATION TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def translate_greedy(sentence, max_len=40):\n",
        "    \"\"\"\n",
        "    Translate an Arabic sentence to English using greedy decoding.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): Arabic input sentence\n",
        "        max_len (int): Maximum output length\n",
        "\n",
        "    Returns:\n",
        "        str: Translated English sentence\n",
        "    \"\"\"\n",
        "    # 1. Normalize Arabic\n",
        "    sentence = normalize_arabic(sentence)\n",
        "\n",
        "    # 2. Encode source with BOS/EOS\n",
        "    src_ids = sp_ar.encode(\n",
        "        sentence,\n",
        "        out_type=int,\n",
        "        add_bos=True,\n",
        "        add_eos=True\n",
        "    )\n",
        "\n",
        "    if len(src_ids) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    inp = tf.constant([src_ids], dtype=tf.int32)\n",
        "\n",
        "    # 3. Decoder starts with BOS\n",
        "    decoded = tf.constant([[BOS_ID]], dtype=tf.int32)\n",
        "\n",
        "    # 4. Autoregressive decoding\n",
        "    for _ in range(max_len):\n",
        "        enc_mask, dec_self_mask, dec_cross_mask = create_masks(inp, decoded)\n",
        "\n",
        "        logits = transformer(\n",
        "            inp=inp,\n",
        "            tar=decoded,\n",
        "            training=False,\n",
        "            enc_padding_mask=enc_mask,\n",
        "            look_ahead_mask=dec_self_mask,\n",
        "            dec_padding_mask=dec_cross_mask\n",
        "        )\n",
        "\n",
        "        next_token = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
        "        next_id = int(next_token.numpy()[0])\n",
        "\n",
        "        if next_id == EOS_ID:\n",
        "            break\n",
        "\n",
        "        decoded = tf.concat([decoded, [[next_id]]], axis=1)\n",
        "\n",
        "    # 5. Remove BOS and decode\n",
        "    output_ids = decoded.numpy()[0][1:]\n",
        "    return sp_en.decode(output_ids.tolist())\n",
        "\n",
        "\n",
        "# Test simple translations\n",
        "import random\n",
        "\n",
        "def load_text_file(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "test_ar_text = load_text_file(\"test.ar\")\n",
        "test_en_text = load_text_file(\"test.en\")  # optional\n",
        "random.seed(42)\n",
        "indices = random.sample(range(len(test_ar_text)), 10)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RANDOM TEST SET TRANSLATION (GREEDY)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in indices:\n",
        "    ar = test_ar_text[i]\n",
        "    ref = test_en_text[i] if i < len(test_en_text) else \"\"\n",
        "    pred = translate_greedy(ar)\n",
        "\n",
        "    print(f\"\\nAR:   {ar}\")\n",
        "    print(f\"PRED: {pred}\")\n",
        "    print(f\"REF:  {ref}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYyw_kx_NRZh"
      },
      "source": [
        "CHECK MODEL WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdzogh3bNfl0",
        "outputId": "3b4281eb-1b5c-4d9e-90b5-2b47041dd02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL WEIGHTS CHECK\n",
            "============================================================\n",
            "\n",
            "Model Statistics:\n",
            "  Total parameters: 11,986,800\n",
            "  Layers with >95% zeros: 0/172\n",
            "  ✅ Good: Weights are properly updated.\n"
          ]
        }
      ],
      "source": [
        "# ========== CHECK MODEL WEIGHTS ==========\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL WEIGHTS CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check if weights are updated (not all zeros)\n",
        "all_weights = transformer.get_weights()\n",
        "zero_counts = 0\n",
        "total_params = 0\n",
        "\n",
        "for i, weight in enumerate(all_weights):\n",
        "    total_params += np.prod(weight.shape)\n",
        "    zero_elements = np.sum(weight == 0)\n",
        "    zero_percentage = (zero_elements / np.prod(weight.shape)) * 100\n",
        "\n",
        "    if zero_percentage > 95:  # إذا كان أكثر من 95% أصفار\n",
        "        zero_counts += 1\n",
        "        print(f\"  ⚠ Layer {i}: {zero_percentage:.1f}% zeros\")\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Layers with >95% zeros: {zero_counts}/{len(all_weights)}\")\n",
        "\n",
        "if zero_counts > len(all_weights) / 2:\n",
        "    print(\"  ❌ PROBLEM: Too many weights are zeros! Training didn't work.\")\n",
        "else:\n",
        "    print(\"  ✅ Good: Weights are properly updated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et4r0WmzweNq"
      },
      "source": [
        "#With beam search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "M0QSJcGc-3Yo"
      },
      "outputs": [],
      "source": [
        "def beam_search_translate_fixed(sentence, beam_width=4, max_len=40, length_penalty=0.6):\n",
        "    \"\"\"\n",
        "    Improved beam search translation with length normalization.\n",
        "    \"\"\"\n",
        "    # 1. Normalize Arabic\n",
        "    sentence = normalize_arabic(sentence)\n",
        "    src_ids = sp_ar.encode(sentence, out_type=int, add_bos=True, add_eos=True)\n",
        "    if len(src_ids) == 0:\n",
        "        return []\n",
        "\n",
        "    # 2. Encoder input\n",
        "    encoder_input = tf.constant([src_ids], dtype=tf.int32)\n",
        "    enc_padding_mask = create_padding_mask(encoder_input)\n",
        "    encoder_output = transformer.encoder(encoder_input, training=False, mask=enc_padding_mask)\n",
        "\n",
        "    # 3. Initialize beams\n",
        "    beams = [{\n",
        "        'sequence': [BOS_ID],\n",
        "        'score': 0.0,\n",
        "        'decoder_input': tf.constant([[BOS_ID]], dtype=tf.int32)\n",
        "    }]\n",
        "\n",
        "    for step in range(max_len):\n",
        "        all_candidates = []\n",
        "\n",
        "        for beam in beams:\n",
        "            if beam['sequence'][-1] == EOS_ID:\n",
        "                all_candidates.append(beam)\n",
        "                continue\n",
        "\n",
        "            # Decoder masks\n",
        "            decoder_input = beam['decoder_input']\n",
        "            look_ahead_mask = create_look_ahead_mask(tf.shape(decoder_input)[1])\n",
        "            look_ahead_mask = tf.minimum(\n",
        "                look_ahead_mask,\n",
        "                create_padding_mask(decoder_input)[:, :, :tf.shape(decoder_input)[1]]\n",
        "            )\n",
        "\n",
        "            # Decoder output\n",
        "            decoder_output = transformer.decoder(\n",
        "                decoder_input,\n",
        "                encoder_output,\n",
        "                training=False,\n",
        "                look_ahead_mask=look_ahead_mask,\n",
        "                padding_mask=enc_padding_mask\n",
        "            )\n",
        "            logits = transformer.final_layer(decoder_output[:, -1, :])\n",
        "            log_probs = tf.nn.log_softmax(logits).numpy()[0]\n",
        "\n",
        "            # Top-k candidates\n",
        "            top_k_ids = np.argsort(log_probs)[-beam_width:][::-1]\n",
        "            for token_id in top_k_ids:\n",
        "                seq = beam['sequence'] + [int(token_id)]\n",
        "                score = beam['score'] + log_probs[token_id]\n",
        "\n",
        "                # Length normalization\n",
        "                norm_score = score / ((5 + len(seq)) ** length_penalty / (6 ** length_penalty))\n",
        "\n",
        "                candidate = {\n",
        "                    'sequence': seq,\n",
        "                    'score': norm_score,\n",
        "                    'decoder_input': tf.concat([beam['decoder_input'], tf.constant([[token_id]], dtype=tf.int32)], axis=1)\n",
        "                }\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        # Keep top beams\n",
        "        all_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
        "        beams = all_candidates[:beam_width]\n",
        "\n",
        "        # Stop if all beams ended with EOS\n",
        "        if all(beam['sequence'][-1] == EOS_ID for beam in beams):\n",
        "            break\n",
        "\n",
        "    # Prepare final results\n",
        "    results = []\n",
        "    for beam in beams:\n",
        "        output_ids = beam['sequence'][1:]  # remove BOS\n",
        "        if output_ids and output_ids[-1] == EOS_ID:\n",
        "            output_ids = output_ids[:-1]\n",
        "        translation = sp_en.decode(output_ids)\n",
        "        results.append({'translation': translation, 'score': beam['score'], 'sequence': beam['sequence']})\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh6uOaVCdEOy",
        "outputId": "56cc9d8f-007a-4eaf-ab5d-cb644e0f0a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam 1 (score=0.9320): hi. how are you?\n",
            "Beam 2 (score=0.9192): hi! how are you?\n",
            "Beam 3 (score=0.8417): hello, how are you?\n",
            "Beam 4 (score=0.8347): welcome to how are you?\n",
            "Beam 5 (score=0.5092): hello are you?\n"
          ]
        }
      ],
      "source": [
        "results = beam_search_translate_fixed(\"مرحبا كيف حالك\", beam_width=5, length_penalty=1.0)\n",
        "for i, r in enumerate(results):\n",
        "    print(f\"Beam {i+1} (score={np.exp(r['score']):.4f}): {r['translation']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bECCZ7a-u-4Q"
      },
      "source": [
        "#BLEU SCORE EVALUATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BETH3Wj1AjzF",
        "outputId": "61b1887e-25f9-4bda-87a3-5ada26a5b5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install sacrebleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FpO3fz7OB2iZ"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_bleu(test_ar, test_en, decode_fn, max_samples=1000):\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for ar, ref in tqdm(list(zip(test_ar, test_en))[:max_samples]):\n",
        "        pred = decode_fn(ar)\n",
        "        predictions.append(pred)\n",
        "        references.append(ref)\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
        "    return bleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTNJ8rBlB9vp",
        "outputId": "8e674d53-03f2-4982-a2eb-36446834b0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [24:54<00:00,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 BLEU (Greedy):\n",
            "BLEU = 25.45 54.1/30.0/19.5/13.2 (BP = 1.000 ratio = 1.028 hyp_len = 6998 ref_len = 6806)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bleu_greedy = evaluate_bleu(\n",
        "    test_ar_text,\n",
        "    test_en_text,\n",
        "    translate_greedy,\n",
        "    max_samples=1000\n",
        ")\n",
        "\n",
        "print(\"🔹 BLEU (Greedy):\")\n",
        "print(bleu_greedy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "q-XD-V9fIjF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb53bd0-846f-4c29-84d4-b2faddf6af72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [57:18<00:00,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 BLEU (Beam Search):\n",
            "BLEU = 24.44 51.7/29.0/18.9/12.6 (BP = 1.000 ratio = 1.109 hyp_len = 7546 ref_len = 6806)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def beam_best(sentence):\n",
        "    results = beam_search_translate_fixed(sentence, beam_width=4)\n",
        "    return results[0][\"translation\"]  # Best beam only\n",
        "\n",
        "bleu_beam = evaluate_bleu(\n",
        "    test_ar_text,\n",
        "    test_en_text,\n",
        "    beam_best,\n",
        "    max_samples=1000\n",
        ")\n",
        "\n",
        "print(\"🔹 BLEU (Beam Search):\")\n",
        "print(bleu_beam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nkqT7Y43VCL"
      },
      "source": [
        "# GUI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "history_data = []\n",
        "\n",
        "def translate_text(source_text, source_lang, target_lang):\n",
        "    \"\"\"\n",
        "    Translate text using the trained model\n",
        "    \"\"\"\n",
        "    if not source_text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        if source_lang == \"Arabic\" and target_lang == \"English\":\n",
        "\n",
        "            translated = translate_greedy(source_text, max_len=40)\n",
        "\n",
        "        else:\n",
        "            translated = \"⚠️ Invalid language selection\"\n",
        "\n",
        "        return translated\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {str(e)}\"\n",
        "\n",
        "def translate_and_save(source_text, source_lang, target_lang):\n",
        "    \"\"\"\n",
        "    Translate text and add to history\n",
        "    \"\"\"\n",
        "    if not source_text.strip():\n",
        "        return \"\", create_history_df()\n",
        "\n",
        "    # Get translation from model\n",
        "    translated = translate_text(source_text, source_lang, target_lang)\n",
        "\n",
        "    # Add to history\n",
        "    history_entry = {\n",
        "        \"Source\": source_lang,\n",
        "        \"Target\": target_lang,\n",
        "        \"Source Text\": source_text,\n",
        "        \"Translation\": translated\n",
        "    }\n",
        "    history_data.insert(0, history_entry)\n",
        "\n",
        "    return translated, create_history_df()\n",
        "\n",
        "def create_history_df():\n",
        "    \"\"\"\n",
        "    Create DataFrame from history for display\n",
        "    \"\"\"\n",
        "    if not history_data:\n",
        "        return pd.DataFrame(columns=[\"Source\", \"Target\", \"Source Text\", \"Translation\"])\n",
        "    return pd.DataFrame(history_data)\n",
        "\n",
        "def clear_history():\n",
        "    \"\"\"\n",
        "    Clear all translation history\n",
        "    \"\"\"\n",
        "    global history_data\n",
        "    history_data = []\n",
        "    return create_history_df()\n",
        "\n",
        "\n",
        "# Enhanced Custom CSS with animations\n",
        "custom_css = \"\"\"\n",
        "@keyframes fadeInDown {\n",
        "    from {\n",
        "        opacity: 0;\n",
        "        transform: translateY(-30px);\n",
        "    }\n",
        "    to {\n",
        "        opacity: 1;\n",
        "        transform: translateY(0);\n",
        "    }\n",
        "}\n",
        "\n",
        "@keyframes fadeInUp {\n",
        "    from {\n",
        "        opacity: 0;\n",
        "        transform: translateY(30px);\n",
        "    }\n",
        "    to {\n",
        "        opacity: 1;\n",
        "        transform: translateY(0);\n",
        "    }\n",
        "}\n",
        "\n",
        "@keyframes slideInLeft {\n",
        "    from {\n",
        "        transform: translateX(-100px);\n",
        "        opacity: 0;\n",
        "    }\n",
        "    to {\n",
        "        transform: translateX(0);\n",
        "        opacity: 1;\n",
        "    }\n",
        "}\n",
        "\n",
        "@keyframes slideInRight {\n",
        "    from {\n",
        "        transform: translateX(100px);\n",
        "        opacity: 0;\n",
        "    }\n",
        "    to {\n",
        "        transform: translateX(0);\n",
        "        opacity: 1;\n",
        "    }\n",
        "}\n",
        "\n",
        "@keyframes bounceIn {\n",
        "    0% {\n",
        "        transform: scale(0.3);\n",
        "        opacity: 0;\n",
        "    }\n",
        "    50% {\n",
        "        transform: scale(1.05);\n",
        "    }\n",
        "    70% {\n",
        "        transform: scale(0.9);\n",
        "    }\n",
        "    100% {\n",
        "        transform: scale(1);\n",
        "        opacity: 1;\n",
        "    }\n",
        "}\n",
        "\n",
        "@keyframes pulse {\n",
        "    0%, 100% {\n",
        "        transform: scale(1);\n",
        "        box-shadow: 0 0 0 0 rgba(102, 126, 234, 0.7);\n",
        "    }\n",
        "    50% {\n",
        "        transform: scale(1.05);\n",
        "        box-shadow: 0 0 0 10px rgba(102, 126, 234, 0);\n",
        "    }\n",
        "}\n",
        "\n",
        "@keyframes gradient {\n",
        "    0% { background-position: 0% 50%; }\n",
        "    50% { background-position: 100% 50%; }\n",
        "    100% { background-position: 0% 50%; }\n",
        "}\n",
        "\n",
        "@keyframes shimmer {\n",
        "    0% { left: -100%; }\n",
        "    100% { left: 100%; }\n",
        "}\n",
        "\n",
        "@keyframes rotate {\n",
        "    from { transform: rotate(0deg); }\n",
        "    to { transform: rotate(360deg); }\n",
        "}\n",
        "\n",
        "@keyframes float {\n",
        "    0%, 100% { transform: translateY(0px); }\n",
        "    50% { transform: translateY(-10px); }\n",
        "}\n",
        "\n",
        "@keyframes glow {\n",
        "    0%, 100% {\n",
        "        box-shadow: 0 0 5px rgba(102, 126, 234, 0.5),\n",
        "                    0 0 10px rgba(102, 126, 234, 0.3);\n",
        "    }\n",
        "    50% {\n",
        "        box-shadow: 0 0 20px rgba(102, 126, 234, 0.8),\n",
        "                    0 0 30px rgba(102, 126, 234, 0.5),\n",
        "                    0 0 40px rgba(102, 126, 234, 0.3);\n",
        "    }\n",
        "}\n",
        "\n",
        "@keyframes rainbow {\n",
        "    0% { filter: hue-rotate(0deg); }\n",
        "    100% { filter: hue-rotate(360deg); }\n",
        "}\n",
        "\n",
        "/* Container animations */\n",
        ".gradio-container {\n",
        "    animation: fadeInUp 1s ease-out;\n",
        "    background: #FFB6C1 !important;\n",
        "    background-size: 200% 200%;\n",
        "}\n",
        "\n",
        "/* Title animations */\n",
        "h1 {\n",
        "    animation: bounceIn 1s ease-out, float 3s ease-in-out infinite;\n",
        "    background: linear-gradient(45deg, #667eea, #764ba2, #f093fb, #4facfe);\n",
        "    background-size: 300% 300%;\n",
        "    -webkit-background-clip: text;\n",
        "    -webkit-text-fill-color: transparent;\n",
        "    background-clip: text;\n",
        "    animation: gradient 5s ease infinite, bounceIn 1s ease-out;\n",
        "    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.1);\n",
        "    text-align: center !important;\n",
        "}\n",
        "\n",
        "h3 {\n",
        "    animation: slideInLeft 0.8s ease-out;\n",
        "    position: relative;\n",
        "}\n",
        "\n",
        "h3::after {\n",
        "    content: '';\n",
        "    position: absolute;\n",
        "    bottom: -5px;\n",
        "    left: 0;\n",
        "    width: 0;\n",
        "    height: 3px;\n",
        "    background: linear-gradient(90deg, #667eea, #764ba2);\n",
        "    animation: expandWidth 1s ease-out forwards;\n",
        "    animation-delay: 0.5s;\n",
        "}\n",
        "\n",
        "@keyframes expandWidth {\n",
        "    to { width: 100%; }\n",
        "}\n",
        "\n",
        "/* Button mega animations */\n",
        "button {\n",
        "    transition: all 0.4s cubic-bezier(0.68, -0.55, 0.265, 1.55);\n",
        "    position: relative;\n",
        "    overflow: hidden;\n",
        "}\n",
        "\n",
        "button:hover {\n",
        "    transform: translateY(-5px) scale(1.05);\n",
        "    box-shadow: 0 10px 25px rgba(102, 126, 234, 0.5);\n",
        "    animation: pulse 1s infinite;\n",
        "}\n",
        "\n",
        "button:active {\n",
        "    transform: translateY(-2px) scale(1.02);\n",
        "    animation: none;\n",
        "}\n",
        "\n",
        "button::before {\n",
        "    content: '';\n",
        "    position: absolute;\n",
        "    top: 50%;\n",
        "    left: 50%;\n",
        "    width: 0;\n",
        "    height: 0;\n",
        "    border-radius: 50%;\n",
        "    background: rgba(255, 255, 255, 0.5);\n",
        "    transform: translate(-50%, -50%);\n",
        "    transition: width 0.6s, height 0.6s;\n",
        "}\n",
        "\n",
        "button:hover::before {\n",
        "    width: 400px;\n",
        "    height: 400px;\n",
        "}\n",
        "\n",
        "button::after {\n",
        "\n",
        "    position: absolute;\n",
        "    top: 50%;\n",
        "    left: 50%;\n",
        "    transform: translate(-50%, -50%) scale(0);\n",
        "    opacity: 0;\n",
        "    transition: all 0.4s ease;\n",
        "}\n",
        "\n",
        "button:hover::after {\n",
        "    transform: translate(-50%, -50%) scale(1);\n",
        "    opacity: 1;\n",
        "}\n",
        "\n",
        "/* Primary button extra effects */\n",
        ".primary {\n",
        "    background: linear-gradient(45deg, #667eea 0%, #764ba2 50%, #f093fb 100%);\n",
        "    background-size: 300% 300%;\n",
        "    animation: gradient 4s ease infinite;\n",
        "    border: none;\n",
        "    color: white;\n",
        "    font-weight: bold;\n",
        "}\n",
        "\n",
        ".primary:hover {\n",
        "    animation: gradient 2s ease infinite, glow 2s ease infinite;\n",
        "}\n",
        "\n",
        "/* Textbox animations */\n",
        "textarea, input[type=\"text\"] {\n",
        "    transition: all 0.4s ease;\n",
        "    animation: fadeInUp 0.6s ease-out;\n",
        "}\n",
        "\n",
        "textarea:focus, input[type=\"text\"]:focus {\n",
        "    box-shadow: 0 0 0 4px rgba(102, 126, 234, 0.3),\n",
        "                0 0 20px rgba(102, 126, 234, 0.4);\n",
        "    transform: scale(1.02);\n",
        "    animation: glow 2s ease infinite;\n",
        "}\n",
        "\n",
        "/* Dropdown animations */\n",
        ".dropdown, select {\n",
        "    transition: all 0.4s ease;\n",
        "    animation: slideInRight 0.8s ease-out;\n",
        "}\n",
        "\n",
        ".dropdown:hover, select:hover {\n",
        "    border-color: #667eea;\n",
        "    transform: translateY(-2px);\n",
        "    box-shadow: 0 5px 15px rgba(102, 126, 234, 0.3);\n",
        "}\n",
        "\n",
        "/* Label animations */\n",
        "label {\n",
        "    animation: fadeInDown 0.8s ease-out;\n",
        "    transition: all 0.3s ease;\n",
        "}\n",
        "\n",
        "label:hover {\n",
        "    color: #667eea;\n",
        "    transform: translateX(5px);\n",
        "}\n",
        "\n",
        "/* Dataframe animations */\n",
        ".dataframe {\n",
        "    animation: fadeInUp 0.8s ease-out;\n",
        "    transition: all 0.3s ease;\n",
        "}\n",
        "\n",
        ".dataframe:hover {\n",
        "    box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);\n",
        "    transform: translateY(-3px);\n",
        "}\n",
        "\n",
        ".dataframe tbody tr {\n",
        "    animation: slideInLeft 0.5s ease-out;\n",
        "    transition: all 0.3s ease;\n",
        "}\n",
        "\n",
        ".dataframe tbody tr:hover {\n",
        "    background: linear-gradient(90deg, rgba(102, 126, 234, 0.1), rgba(118, 75, 162, 0.1));\n",
        "    transform: translateX(5px);\n",
        "}\n",
        "\n",
        ".dataframe thead th {\n",
        "    animation: bounceIn 0.8s ease-out;\n",
        "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "}\n",
        "\n",
        "/* Column animations */\n",
        ".col {\n",
        "    animation: fadeInUp 0.8s ease-out;\n",
        "}\n",
        "\n",
        ".col:nth-child(1) {\n",
        "    animation-delay: 0.1s;\n",
        "}\n",
        "\n",
        ".col:nth-child(2) {\n",
        "    animation-delay: 0.3s;\n",
        "}\n",
        "\n",
        "/* Row animations */\n",
        ".row {\n",
        "    animation: fadeInUp 0.6s ease-out;\n",
        "}\n",
        "\n",
        "/* Shimmer effect for loading */\n",
        ".pending::after {\n",
        "    content: '';\n",
        "    position: absolute;\n",
        "    top: 0;\n",
        "    left: -100%;\n",
        "    width: 100%;\n",
        "    height: 100%;\n",
        "    background: linear-gradient(90deg,\n",
        "        transparent,\n",
        "        rgba(255, 255, 255, 0.4),\n",
        "        transparent\n",
        "    );\n",
        "    animation: shimmer 2s infinite;\n",
        "}\n",
        "\n",
        "/* Add sparkle effect */\n",
        "@keyframes sparkle {\n",
        "    0%, 100% { opacity: 0; transform: scale(0); }\n",
        "    50% { opacity: 1; transform: scale(1); }\n",
        "}\n",
        "\n",
        "/* Emoji animation in title */\n",
        "h1::before {\n",
        "    content: '🌐';\n",
        "    display: inline-block;\n",
        "    margin-right: 10px;\n",
        "    animation: rotate 4s linear infinite, float 2s ease-in-out infinite;\n",
        "}\n",
        "\n",
        "/* Add border animation */\n",
        "@keyframes borderGlow {\n",
        "    0%, 100% { border-color: #667eea; }\n",
        "    50% { border-color: #764ba2; }\n",
        "}\n",
        "\n",
        "textarea, input, select {\n",
        "    animation: fadeInUp 0.6s ease-out, borderGlow 3s ease infinite;\n",
        "}\n",
        "\n",
        "/* Smooth page transitions */\n",
        "* {\n",
        "    transition: all 0.3s ease;\n",
        "}\n",
        "\n",
        "/* Add background pattern animation */\n",
        "body {\n",
        "    position: relative;\n",
        "}\n",
        "\n",
        "body::before {\n",
        "    content: '';\n",
        "    position: fixed;\n",
        "    top: 0;\n",
        "    left: 0;\n",
        "    width: 100%;\n",
        "    height: 100%;\n",
        "    background:\n",
        "        radial-gradient(circle at 20% 50%, rgba(102, 126, 234, 0.1) 0%, transparent 50%),\n",
        "        radial-gradient(circle at 80% 80%, rgba(118, 75, 162, 0.1) 0%, transparent 50%);\n",
        "    animation: float 20s ease-in-out infinite;\n",
        "    pointer-events: none;\n",
        "    z-index: -1;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Our Translator\", css=custom_css) as demo:\n",
        "    gr.Markdown(\"# 🌐 Our Translator\")\n",
        "    gr.Markdown(\"Powered by Transformer Neural Machine Translation Model \")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            # Translation Section\n",
        "            gr.Markdown(\"### Translation\")\n",
        "\n",
        "            with gr.Row():\n",
        "                source_lang = gr.Dropdown(\n",
        "                    choices=[\"English\", \"Arabic\"],\n",
        "                    value=\"Arabic\",\n",
        "                    label=\"Source Language\"\n",
        "                )\n",
        "                target_lang = gr.Dropdown(\n",
        "                    choices=[\"English\", \"Arabic\"],\n",
        "                    value= \"English\",\n",
        "                    label=\"Target Language\"\n",
        "                )\n",
        "\n",
        "            source_text = gr.Textbox(\n",
        "                label=\"Source Text\",\n",
        "                placeholder=\"Enter text to translate...\",\n",
        "                lines=5\n",
        "            )\n",
        "\n",
        "            translate_btn = gr.Button(\"Translate\", variant=\"primary\")\n",
        "\n",
        "            translated_text = gr.Textbox(\n",
        "                label=\"Translation\",\n",
        "                placeholder=\"Translation will appear here...\",\n",
        "                lines=5,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            # History Section\n",
        "            gr.Markdown(\"### Translation History\")\n",
        "            clear_btn = gr.Button(\"Clear History\", variant=\"secondary\")\n",
        "\n",
        "    # History Table (full width)\n",
        "    with gr.Row():\n",
        "        history_table = gr.Dataframe(\n",
        "            value=create_history_df(),\n",
        "            label=\"History\",\n",
        "            headers=[ \"Source Text\", \"Translation\"],\n",
        "            interactive=False,\n",
        "            wrap=True\n",
        "        )\n",
        "\n",
        "    # Event handlers\n",
        "    translate_btn.click(\n",
        "        fn=translate_and_save,\n",
        "        inputs=[source_text, source_lang, target_lang],\n",
        "        outputs=[translated_text, history_table]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_history,\n",
        "        inputs=[],\n",
        "        outputs=[history_table]\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "20X4wFUXWDGm",
        "outputId": "aa35efbc-809a-4a67-8664-4e43c77caf1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3669616885.py:430: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(title=\"Our Translator\", css=custom_css) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://60f2281b66179a53bb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://60f2281b66179a53bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}